import json
import time

import pandas as pd
import seaborn as sns
import streamlit as st

import src.expectation_manager as em
import src.gx_utils as gx_utils

st.set_page_config(page_title="Load Configuration", page_icon="‚öôÔ∏è")

st.markdown("# Load Configuration")
st.sidebar.header("Configuration")
openai_api_key = st.sidebar.text_input("OpenAI API Key", key="api_key", type="password")
st.write(
    """
    This is a load test of a configuration file for data contracts.
    You need to upload two files:
    1. A sample data file (.csv format)
    2. A data contract generated by the tool (.yaml format)

    After uploading the files, the application will process them and show a table with the data and the configuration.
    """
)

uploaded_files = st.file_uploader(
    "Upload sample data (csv) and the configuration file of the data contract (json or yaml):",
    type=["csv", "json", "yaml"], accept_multiple_files=True
)

sample_data_file = None
sample_data_file_count = 0
data_contract_configuration_file = None
data_contract_configuration_file_count = 0

if uploaded_files is not None:
    for file in uploaded_files:
        if file.type == "text/csv":
            #st.write(f"Sample data uploaded: {file.name}")
            sample_data_file = file
            sample_data_file_count += 1
        elif file.type == "text/json" or file.type == "application/x-yaml":
            #st.write(f"Data contract configuration file uploaded: {file.name}")
            data_contract_configuration_file = file
            data_contract_configuration_file_count += 1
        if sample_data_file_count > 1 or data_contract_configuration_file_count > 1:
            st.error(
                "Please upload only one sample data (CSV) file and one data contract "
                "configuration (JSON, YAML) file."
            )

# Check if both files are uploaded and if not tell the user what is missing with a warning
if sample_data_file_count == 0 and data_contract_configuration_file_count == 0:
    st.warning("Please upload a sample data file in CSV format and a data contract configuration file in JSON or YAML format.")
elif data_contract_configuration_file_count == 0 and sample_data_file_count == 1:
    st.warning("Please upload a data contract configuration file in JSON or YAML format.")
elif sample_data_file_count == 0 and data_contract_configuration_file_count == 1:
    st.warning("Please upload a sample data file in CSV format.")


if sample_data_file_count == 1 and data_contract_configuration_file_count == 1:

    # Show CSV data in a table
    df = pd.read_csv(sample_data_file)
    st.dataframe(df)

    # Load expectations yaml as a Pydantic model and show in a table
    pydantic_contract = em.load_suite_yaml(data_contract_configuration_file, is_file=False)

    # Show the data contract configuration in a table
    st.header("Data Contract Configuration:")

    contract_dict = json.loads(pydantic_contract.model_dump_json(indent=2))
    st.dataframe(
        contract_dict['expectations'],
        hide_index=True,
        column_config={
            'id': 'Rule Name',
            'expectation': 'Params',
            'severity': 'Severity',
            'source': 'Source',
            'description': 'Description',
        },
        column_order=('id', 'expectation', 'severity', 'source', 'description', 'enabled'),
        width="stretch"
    )

    # Allow user to validate the configuration
    with st.spinner("Validating configuration..."):
        time.sleep(1)  # Simulate some processing time
        try:
            validation_results = gx_utils.run_gx_dataset_validation(
                em.pydantic_to_gx(pydantic_contract), df
            )
            if validation_results.success:
                st.success("Dataset passed!!")
            else:
                st.error("Dataset does not meet expectations.")
                analysis = gx_utils.analyze_validation_results(validation_results)

                # Show an the analysis of errors
                fail_analysis = gx_utils.get_failed_expectations_summary(validation_results)

                # Display failure summary metrics
                st.subheader("üìä Failure Summary")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Total Failures", fail_analysis['total_failures'])
                with col2:
                    critical_failures = len(fail_analysis['failure_summary']['critical_failures'])
                    st.metric("Critical Failures", critical_failures)
                with col3:
                    warning_failures = len(fail_analysis['failure_summary']['warning_failures'])
                    st.metric("Warning Failures", warning_failures)
                with col4:
                    info_failures = len(fail_analysis['failure_summary']['info_failures'])
                    st.metric("Info Failures", info_failures)
                    #t.metric("Failed Columns", len(fail_analysis['failure_summary']['failed_by_column']))

                # Display failed expectations in a table
                st.subheader("‚ùå Failed Expectations")
                failed_expectations_df = pd.DataFrame([
                    {
                        'Expectation ID': exp['expectation_id'],
                        'Type': exp['expectation_type'],
                        'Column': exp['column'],
                        'Severity': exp['severity'].upper(),
                        'Invalid %': f"{exp['failure_info']['invalid_percentage']:.1f}%",
                        'Invalid Count': exp['failure_info']['invalid_count'],
                        'Description': exp['description'][:100] + "..." if len(exp['description']) > 100 else exp['description']
                    }
                    for exp in fail_analysis['failed_expectations']
                ])

                st.dataframe(
                    failed_expectations_df,
                    use_container_width=True,
                    column_config={
                        'Severity': st.column_config.TextColumn(
                            width="small",
                        ),
                        'Invalid %': st.column_config.TextColumn(
                            width="small",
                        ),
                        'Invalid Count': st.column_config.NumberColumn(
                            width="small",
                        )
                    }
                )

                # Show failure breakdown by type and column
                st.subheader("üìà Failure Analysis")
                col1, col2 = st.columns(2)

                with col1:
                    failure_by_type_df = pd.DataFrame([
                        {'Expectation Type': exp_type, 'Count': count}
                        for exp_type, count in fail_analysis['failure_summary']['failed_by_type'].items()
                    ])
                    import matplotlib.pyplot as plt
                    import plotly.express as px

                    fig = px.bar(
                        failure_by_type_df,
                        y="Expectation Type",
                        x="Count",
                        orientation="h",
                        color="Expectation Type",
                        text="Count"
                    )
                    fig.update_layout(
                        title="Failures by Expectation Type",
                        xaxis_title="Failure Count",
                        yaxis_title="Expectation Type",
                        xaxis=dict(
                            tickmode='linear',
                            tick0=0,
                            dtick=1
                        ),
                        height=400
                    )
                    fig.update_traces(textposition='outside')
                    st.plotly_chart(fig, use_container_width=True)

                with col2:
                    severity_counts = {
                        "Critical": len(fail_analysis['failure_summary']['critical_failures']),
                        "Warning": len(fail_analysis['failure_summary']['warning_failures']),
                        "Info": len(fail_analysis['failure_summary']['info_failures']),
                    }
                    severity_df = pd.DataFrame({
                        "Severity": list(severity_counts.keys()),
                        "Count": list(severity_counts.values())
                    })
                    pie_fig = px.pie(
                        severity_df,
                        names="Severity",
                        values="Count",
                        color="Severity",
                        title="Failures by Severity",
                        hole=0.4
                    )
                    pie_fig.update_traces(textinfo='percent+label')
                    st.plotly_chart(pie_fig, use_container_width=True)

                # Detailed view of specific failures
                st.subheader("üîç Detailed Failure Information")
                selected_expectation = st.selectbox(
                    "Select an expectation to see detailed failure information:",
                    options=[exp['expectation_id'] for exp in fail_analysis['failed_expectations']],
                    key="expectation_selector"
                )

                if selected_expectation:
                    selected_exp = next(
                        exp for exp in fail_analysis['failed_expectations']
                        if exp['expectation_id'] == selected_expectation
                    )

                    info_cols = st.columns(5)
                    info_cols[0].markdown(f"**Expectation Type**<br>{selected_exp['expectation_type']}", unsafe_allow_html=True)
                    info_cols[1].markdown(f"**Column**<br>{selected_exp['column']}", unsafe_allow_html=True)
                    info_cols[2].markdown(f"**Severity**<br>{selected_exp['severity'].upper()}", unsafe_allow_html=True)
                    info_cols[3].markdown(f"**Source**<br>{selected_exp['source']}", unsafe_allow_html=True)
                    info_cols[4].markdown(f"**Description**<br>{selected_exp['description']}", unsafe_allow_html=True)

                    # Show failure details
                    failure_info = selected_exp['failure_info']
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Total Elements", failure_info['total_elements'])
                    with col2:
                        st.metric("Invalid Count", failure_info['invalid_count'])
                    with col3:
                        st.metric("Invalid Percentage", f"{failure_info['invalid_percentage']:.1f}%")

                    # Show invalid values if available
                    if failure_info.get('invalid_values'):
                        st.write("**Invalid Values Found:**")
                        invalid_values_df = pd.DataFrame([
                            {'Value': str(val), 'Count': count.get('count', 1) if isinstance(count, dict) else 1}
                            for val, count in zip(
                                failure_info['invalid_values'],
                                failure_info.get('value_counts', failure_info['invalid_values'])
                            )
                        ])
                        st.dataframe(invalid_values_df, hide_index=True)

                    # Show invalid row indices
                    if failure_info.get('invalid_indices'):
                        st.write("**Rows with Issues (indices):**")
                        st.write(", ".join(map(str, failure_info['invalid_indices'])))

                        # Show the actual problematic rows
                        if len(failure_info['invalid_indices']) > 0:
                            st.write("**Problematic Data Rows:**")
                            problematic_rows = df.iloc[failure_info['invalid_indices']]
                            st.dataframe(problematic_rows, use_container_width=True)


        except Exception as e:
            st.error(f"Configuration is invalid: {e}")

    #test_data = {
    #    "data_contract": {
    #        "expectations": [
    #            {"name": "field1", "type": "string", "constraints": '{"max_length": 255}'},
    #            {"name": "field2", "type": "integer", "constraints": '{"min_value": 0}'}
    #        ],
    #        "primary_key": ["field1"]
    #    }
    #}
    #edited_data = st.data_editor(
    #    test_data['data_contract']['expectations'], hide_index=True,
    #    num_rows="dynamic", width="stretch"
    #)

    #st.success("Configuration tested successfully!")
    #df = pd.DataFrame(edited_data)

    ## st.download_button(
    ##     label="Download CSV",
    ##     data=df.to_csv().encode("utf-8"),
    ##     file_name="data.csv",
    ##     mime="text/csv",
    ##     icon=":material/download:"
    ## )
    #st.download_button(
    #    label="Download JSON",
    #    data=df.to_json().encode("utf-8"),
    #    file_name="data.json",
    #    mime="application/json",
    #    icon=":material/download:"
    #)
